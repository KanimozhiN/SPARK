{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "ead319e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "13e29b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf # @udf(\"integer\") def myfunc(x,y): return x - y\n",
    "from pyspark.sql import functions as F # stddev format_number date_format, dayofyear, when\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b9f214e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('numpy', '1.22.3'), ('pandas', '1.4.0'), ('pyspark', '3.2.1')]\n"
     ]
    }
   ],
   "source": [
    "print([(x.__name__,x.__version__) for x in [np, pd, pyspark]])\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName('stroke').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bb6ff39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, VectorIndexer,\n",
    "                               OneHotEncoder, StringIndexer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4446bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5110\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "| 9046|  Male|67.0|           0|            1|         Yes|      Private|         Urban|           228.69|36.6|formerly smoked|     1|\n",
      "|51676|Female|61.0|           0|            0|         Yes|Self-employed|         Rural|           202.21| N/A|   never smoked|     1|\n",
      "|31112|  Male|80.0|           0|            1|         Yes|      Private|         Rural|           105.92|32.5|   never smoked|     1|\n",
      "|60182|Female|49.0|           0|            0|         Yes|      Private|         Urban|           171.23|34.4|         smokes|     1|\n",
      "| 1665|Female|79.0|           1|            0|         Yes|Self-employed|         Rural|           174.12|  24|   never smoked|     1|\n",
      "|56669|  Male|81.0|           0|            0|         Yes|      Private|         Urban|           186.21|  29|formerly smoked|     1|\n",
      "|53882|  Male|74.0|           1|            1|         Yes|      Private|         Rural|            70.09|27.4|   never smoked|     1|\n",
      "|10434|Female|69.0|           0|            0|          No|      Private|         Urban|            94.39|22.8|   never smoked|     1|\n",
      "|27419|Female|59.0|           0|            0|         Yes|      Private|         Rural|            76.15| N/A|        Unknown|     1|\n",
      "|60491|Female|78.0|           0|            0|         Yes|      Private|         Urban|            58.57|24.2|        Unknown|     1|\n",
      "|12109|Female|81.0|           1|            0|         Yes|      Private|         Rural|            80.43|29.7|   never smoked|     1|\n",
      "|12095|Female|61.0|           0|            1|         Yes|     Govt_job|         Rural|           120.46|36.8|         smokes|     1|\n",
      "|12175|Female|54.0|           0|            0|         Yes|      Private|         Urban|           104.51|27.3|         smokes|     1|\n",
      "| 8213|  Male|78.0|           0|            1|         Yes|      Private|         Urban|           219.84| N/A|        Unknown|     1|\n",
      "| 5317|Female|79.0|           0|            1|         Yes|      Private|         Urban|           214.09|28.2|   never smoked|     1|\n",
      "|58202|Female|50.0|           1|            0|         Yes|Self-employed|         Rural|           167.41|30.9|   never smoked|     1|\n",
      "|56112|  Male|64.0|           0|            1|         Yes|      Private|         Urban|           191.61|37.5|         smokes|     1|\n",
      "|34120|  Male|75.0|           1|            0|         Yes|      Private|         Urban|           221.29|25.8|         smokes|     1|\n",
      "|27458|Female|60.0|           0|            0|          No|      Private|         Urban|            89.22|37.8|   never smoked|     1|\n",
      "|25226|  Male|57.0|           0|            1|          No|     Govt_job|         Urban|           217.08| N/A|        Unknown|     1|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('healthcare-dataset-stroke-data.csv',header=True,inferSchema=True)\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e792e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: string (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- stroke: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d7b051be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "| id|gender|age|hypertension|heart_disease|ever_married|work_type|Residence_type|avg_glucose_level|bmi|smoking_status|stroke|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "|  0|     0|  0|           0|            0|           0|        0|             0|                0|  0|             0|     0|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "75cae7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| bmi|count|\n",
      "+----+-----+\n",
      "|34.4|   18|\n",
      "|47.5|    3|\n",
      "|20.5|   18|\n",
      "|38.3|    2|\n",
      "|45.4|    4|\n",
      "|38.5|    7|\n",
      "|26.5|   30|\n",
      "|  51|    1|\n",
      "|48.1|    1|\n",
      "|29.4|   30|\n",
      "|16.6|    8|\n",
      "|14.2|    4|\n",
      "|12.8|    1|\n",
      "|26.7|   37|\n",
      "|17.1|   12|\n",
      "|36.1|    7|\n",
      "|30.1|   26|\n",
      "|40.1|   10|\n",
      "|  54|    1|\n",
      "|  15|    2|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('bmi').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "de413cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(bmi)|\n",
      "+------------------+\n",
      "|28.893236911794673|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'bmi': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "67d0b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|id   |gender|age |hypertension|heart_disease|ever_married|work_type    |Residence_type|avg_glucose_level|bmi |smoking_status |stroke|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|9046 |Male  |67.0|0           |1            |Yes         |Private      |Urban         |228.69           |36.6|formerly smoked|1     |\n",
      "|51676|Female|61.0|0           |0            |Yes         |Self-employed|Rural         |202.21           |29  |never smoked   |1     |\n",
      "|31112|Male  |80.0|0           |1            |Yes         |Private      |Rural         |105.92           |32.5|never smoked   |1     |\n",
      "|60182|Female|49.0|0           |0            |Yes         |Private      |Urban         |171.23           |34.4|smokes         |1     |\n",
      "|1665 |Female|79.0|1           |0            |Yes         |Self-employed|Rural         |174.12           |24  |never smoked   |1     |\n",
      "|56669|Male  |81.0|0           |0            |Yes         |Private      |Urban         |186.21           |29  |formerly smoked|1     |\n",
      "|53882|Male  |74.0|1           |1            |Yes         |Private      |Rural         |70.09            |27.4|never smoked   |1     |\n",
      "|10434|Female|69.0|0           |0            |No          |Private      |Urban         |94.39            |22.8|never smoked   |1     |\n",
      "|27419|Female|59.0|0           |0            |Yes         |Private      |Rural         |76.15            |29  |Unknown        |1     |\n",
      "|60491|Female|78.0|0           |0            |Yes         |Private      |Urban         |58.57            |24.2|Unknown        |1     |\n",
      "|12109|Female|81.0|1           |0            |Yes         |Private      |Rural         |80.43            |29.7|never smoked   |1     |\n",
      "|12095|Female|61.0|0           |1            |Yes         |Govt_job     |Rural         |120.46           |36.8|smokes         |1     |\n",
      "|12175|Female|54.0|0           |0            |Yes         |Private      |Urban         |104.51           |27.3|smokes         |1     |\n",
      "|8213 |Male  |78.0|0           |1            |Yes         |Private      |Urban         |219.84           |29  |Unknown        |1     |\n",
      "|5317 |Female|79.0|0           |1            |Yes         |Private      |Urban         |214.09           |28.2|never smoked   |1     |\n",
      "|58202|Female|50.0|1           |0            |Yes         |Self-employed|Rural         |167.41           |30.9|never smoked   |1     |\n",
      "|56112|Male  |64.0|0           |1            |Yes         |Private      |Urban         |191.61           |37.5|smokes         |1     |\n",
      "|34120|Male  |75.0|1           |0            |Yes         |Private      |Urban         |221.29           |25.8|smokes         |1     |\n",
      "|27458|Female|60.0|0           |0            |No          |Private      |Urban         |89.22            |37.8|never smoked   |1     |\n",
      "|25226|Male  |57.0|0           |1            |No          |Govt_job     |Urban         |217.08           |29  |Unknown        |1     |\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "df.withColumn('bmi', \n",
    "    when(df.bmi.endswith('N/A'),regexp_replace(df.bmi,'N/A','29')) \\\n",
    "   .otherwise(df.bmi)) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e523e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|  249|\n",
      "|     0| 4861|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "67f1b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df.filter(df['stroke'] == 0)\n",
    "df_b = df.filter(df['stroke'] == 1)\n",
    "\n",
    "a_count = df_a.count()\n",
    "b_count = df_b.count() \n",
    "ratio = a_count / b_count\n",
    "\n",
    "df_b_oversampled = df_b.sample(withReplacement=True, fraction=ratio, seed=1)\n",
    "df = df_a.unionAll(df_b_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f2a8c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|30669|  Male| 3.0|           0|            0|          No|     children|         Rural|            95.12|  18|        Unknown|     0|\n",
      "|30468|  Male|58.0|           1|            0|         Yes|      Private|         Urban|            87.96|39.2|   never smoked|     0|\n",
      "|16523|Female| 8.0|           0|            0|          No|      Private|         Urban|           110.89|17.6|        Unknown|     0|\n",
      "|56543|Female|70.0|           0|            0|         Yes|      Private|         Rural|            69.04|35.9|formerly smoked|     0|\n",
      "|46136|  Male|14.0|           0|            0|          No| Never_worked|         Rural|           161.28|19.1|        Unknown|     0|\n",
      "|32257|Female|47.0|           0|            0|         Yes|      Private|         Urban|           210.95|50.1|        Unknown|     0|\n",
      "|52800|Female|52.0|           0|            0|         Yes|      Private|         Urban|            77.59|17.7|formerly smoked|     0|\n",
      "|41413|Female|75.0|           0|            1|         Yes|Self-employed|         Rural|           243.53|  27|   never smoked|     0|\n",
      "|15266|Female|32.0|           0|            0|         Yes|      Private|         Rural|            77.67|32.3|         smokes|     0|\n",
      "|28674|Female|74.0|           1|            0|         Yes|Self-employed|         Urban|           205.84|54.6|   never smoked|     0|\n",
      "|10460|Female|79.0|           0|            0|         Yes|     Govt_job|         Urban|            77.08|  35|        Unknown|     0|\n",
      "|64908|  Male|79.0|           0|            1|         Yes|      Private|         Urban|            57.08|  22|formerly smoked|     0|\n",
      "|63884|Female|37.0|           0|            0|         Yes|      Private|         Rural|           162.96|39.4|   never smoked|     0|\n",
      "|37893|Female|37.0|           0|            0|         Yes|      Private|         Rural|             73.5|26.1|formerly smoked|     0|\n",
      "|67855|Female|40.0|           0|            0|         Yes|      Private|         Rural|            95.04|42.4|   never smoked|     0|\n",
      "|25774|  Male|35.0|           0|            0|          No|      Private|         Rural|            85.37|  33|   never smoked|     0|\n",
      "|19584|Female|20.0|           0|            0|          No|      Private|         Urban|            84.62|19.7|         smokes|     0|\n",
      "|24447|Female|42.0|           0|            0|         Yes|      Private|         Rural|            82.67|22.5|   never smoked|     0|\n",
      "|49589|Female|44.0|           0|            0|         Yes|     Govt_job|         Urban|            57.33|24.6|         smokes|     0|\n",
      "|17986|Female|79.0|           0|            1|         Yes|Self-employed|         Urban|            67.84|25.2|         smokes|     0|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "de8bfd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     0| 4861|\n",
      "|     1| 4710|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "4ff71200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| bmi|count|\n",
      "+----+-----+\n",
      "|20.5|   34|\n",
      "|34.4|   54|\n",
      "|38.3|    2|\n",
      "|45.4|    4|\n",
      "|47.5|   24|\n",
      "|38.5|    7|\n",
      "|  51|    1|\n",
      "|26.5|   41|\n",
      "|48.1|    1|\n",
      "|29.4|   46|\n",
      "|16.6|    8|\n",
      "|14.2|    4|\n",
      "|12.8|    1|\n",
      "|26.7|   54|\n",
      "|17.1|   12|\n",
      "|36.1|    7|\n",
      "|30.1|   26|\n",
      "|40.1|   10|\n",
      "|  54|    1|\n",
      "|  15|    2|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('bmi').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "11d1a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "64877cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|  Male| 3.0|           0|            0|          No|     children|         Rural|            95.12|  18|        Unknown|     0|\n",
      "|  Male|58.0|           1|            0|         Yes|      Private|         Urban|            87.96|39.2|   never smoked|     0|\n",
      "|Female| 8.0|           0|            0|          No|      Private|         Urban|           110.89|17.6|        Unknown|     0|\n",
      "|Female|70.0|           0|            0|         Yes|      Private|         Rural|            69.04|35.9|formerly smoked|     0|\n",
      "|  Male|14.0|           0|            0|          No| Never_worked|         Rural|           161.28|19.1|        Unknown|     0|\n",
      "|Female|47.0|           0|            0|         Yes|      Private|         Urban|           210.95|50.1|        Unknown|     0|\n",
      "|Female|52.0|           0|            0|         Yes|      Private|         Urban|            77.59|17.7|formerly smoked|     0|\n",
      "|Female|75.0|           0|            1|         Yes|Self-employed|         Rural|           243.53|  27|   never smoked|     0|\n",
      "|Female|32.0|           0|            0|         Yes|      Private|         Rural|            77.67|32.3|         smokes|     0|\n",
      "|Female|74.0|           1|            0|         Yes|Self-employed|         Urban|           205.84|54.6|   never smoked|     0|\n",
      "|Female|79.0|           0|            0|         Yes|     Govt_job|         Urban|            77.08|  35|        Unknown|     0|\n",
      "|  Male|79.0|           0|            1|         Yes|      Private|         Urban|            57.08|  22|formerly smoked|     0|\n",
      "|Female|37.0|           0|            0|         Yes|      Private|         Rural|           162.96|39.4|   never smoked|     0|\n",
      "|Female|37.0|           0|            0|         Yes|      Private|         Rural|             73.5|26.1|formerly smoked|     0|\n",
      "|Female|40.0|           0|            0|         Yes|      Private|         Rural|            95.04|42.4|   never smoked|     0|\n",
      "|  Male|35.0|           0|            0|          No|      Private|         Rural|            85.37|  33|   never smoked|     0|\n",
      "|Female|20.0|           0|            0|          No|      Private|         Urban|            84.62|19.7|         smokes|     0|\n",
      "|Female|42.0|           0|            0|         Yes|      Private|         Rural|            82.67|22.5|   never smoked|     0|\n",
      "|Female|44.0|           0|            0|         Yes|     Govt_job|         Urban|            57.33|24.6|         smokes|     0|\n",
      "|Female|79.0|           0|            1|         Yes|Self-employed|         Urban|            67.84|25.2|         smokes|     0|\n",
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "57a004d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, VectorIndexer,\n",
    "                               OneHotEncoder, StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7befc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='gender', outputCol='gender_index')\n",
    "\n",
    "gender_encoder = OneHotEncoder(inputCol='gender_index', outputCol='gender_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "4a4c84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_indexer = StringIndexer(inputCol='ever_married', outputCol='ever_married_index')\n",
    "\n",
    "marital_encoder = OneHotEncoder(inputCol='ever_married_index', outputCol='ever_married_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "9ec7a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_indexer = StringIndexer(inputCol='work_type', outputCol='work_type_index')\n",
    "\n",
    "work_encoder = OneHotEncoder(inputCol='work_type_index', outputCol='work_type_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "5d0f94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Residence_indexer = StringIndexer(inputCol='Residence_type', outputCol='Residence_type_index')\n",
    "\n",
    "Residence_encoder = OneHotEncoder(inputCol='Residence_type_index', outputCol='Residence_type_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e4b0c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking_indexer = StringIndexer(inputCol='smoking_status', outputCol='smoking_status_index')\n",
    "\n",
    "smoking_encoder = OneHotEncoder(inputCol='smoking_status_index', outputCol='smoking_status_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "a97bc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = df.withColumn(\"bmi\", df[\"bmi\"].cast(IntegerType()))\n",
    "df=df.drop(\"bmi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "124e6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- stroke: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "d56cdeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'age',\n",
       " 'hypertension',\n",
       " 'heart_disease',\n",
       " 'ever_married',\n",
       " 'work_type',\n",
       " 'Residence_type',\n",
       " 'avg_glucose_level',\n",
       " 'smoking_status',\n",
       " 'stroke']"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "6257035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"gender_vec\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married_vec\",\"work_type_vec\",\"Residence_type_vec\",\"avg_glucose_level\",\"smoking_status_vec\"],\n",
    "                           outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e8a45390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "e2306f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features',labelCol='stroke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ea5b899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "2eb44cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer,marital_indexer,work_indexer,Residence_indexer,smoking_status_indexer,\n",
    "                            gender_encoder,marital_encoder,work_encoder,Residence_encoder,smoking_status_encoder,\n",
    "                            assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "550571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "3d1f217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "845ceab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|stroke|prediction|\n",
      "+------+----------+\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = lr_model.transform(test)\n",
    "results['stroke','prediction'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "2ae7fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e33ba132",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='stroke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "32a3309b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8733.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 693.0 failed 1 times, most recent failure: Lost task 0.0 in stage 693.0 (TID 797) (DESKTOP-0UH1SHS executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3816/0x00000001015f1840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Unseen label: Other. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:151)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:189)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:178)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:180)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:180)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:272)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:103)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:123)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:102)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3816/0x00000001015f1840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen label: Other. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-363-120d59db6e9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mauc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr_evaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8733.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 693.0 failed 1 times, most recent failure: Lost task 0.0 in stage 693.0 (TID 797) (DESKTOP-0UH1SHS executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3816/0x00000001015f1840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Unseen label: Other. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:151)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:189)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:178)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:180)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:180)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:272)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:103)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:123)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:102)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3816/0x00000001015f1840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen label: Other. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "auc=lr_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "dd66c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209f8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
